{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from catalyst.data.augmentor import Augmentor\n",
    "from catalyst.utils.factory import UtilsFactory\n",
    "from catalyst.models.segmentation import UNet\n",
    "\n",
    "from varian.models.segnet import SegNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1488)\n",
    "n_images = 500\n",
    "bs = 4\n",
    "n_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [np.random.uniform(high=256, size=(512, 512)).astype(np.uint8)\n",
    "          for x in range(n_images)]\n",
    "masks = [(np.random.uniform(high=1, size=(512, 512)) > 0.5).astype(float)\n",
    "         for x in range(n_images)]\n",
    "data = list(zip(images, masks))\n",
    "\n",
    "train_data = data[:n_images // 2]\n",
    "valid_data = data[n_images // 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Saving mock data\n",
    "# data_dir = Path('..') / 'data'\n",
    "\n",
    "# np.save(data_dir / 'mock_data_images', np.stack(images))\n",
    "# np.save(data_dir / 'mock_data_masks', np.stack(masks))\n",
    "\n",
    "# (data_dir / 'mock_data_images').mkdir(exist_ok=True)\n",
    "# (data_dir / 'mock_data_images' / 'train').mkdir(exist_ok=True)\n",
    "# (data_dir / 'mock_data_images' / 'valid').mkdir(exist_ok=True)\n",
    "\n",
    "# (data_dir / 'mock_data_masks').mkdir(exist_ok=True)\n",
    "# (data_dir / 'mock_data_masks' / 'train').mkdir(exist_ok=True)\n",
    "# (data_dir / 'mock_data_masks' / 'valid').mkdir(exist_ok=True)\n",
    "\n",
    "# for i, (image, mask) in enumerate(train_data):\n",
    "#     Image.fromarray(image).save(data_dir / 'mock_data_images' / 'train' / (str(i) + '.tiff'))\n",
    "#     Image.fromarray(mask).save(data_dir / 'mock_data_masks' / 'train' / (str(i) + '.tiff'))\n",
    "    \n",
    "# for i, (image, mask) in enumerate(valid_data):\n",
    "#     Image.fromarray(image).save(data_dir / 'mock_data_images' / 'valid' / (str(i) + '.tiff'))\n",
    "#     Image.fromarray(mask).save(data_dir / 'mock_data_masks' / 'valid' / (str(i) + '.tiff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    # TODO specify augmentations (e.g. histogram normalization)\n",
    "    Augmentor(\n",
    "        dict_key=\"features\",\n",
    "        augment_fn=lambda x: \\\n",
    "            torch.from_numpy(x.copy().astype(np.float32) / 256.).unsqueeze_(0).float()),\n",
    "    Augmentor(\n",
    "        dict_key=\"features\",\n",
    "        augment_fn=lambda x: np.vstack([x, x, x])),\n",
    "    Augmentor(\n",
    "        dict_key=\"targets\",\n",
    "        augment_fn=lambda x: \\\n",
    "            torch.from_numpy(x.copy()).unsqueeze_(0).float()),\n",
    "])\n",
    "\n",
    "open_fn = lambda x: {\"features\": x[0], \"targets\": x[1]}\n",
    "\n",
    "\n",
    "train_loader = UtilsFactory.create_loader(\n",
    "    train_data, \n",
    "    open_fn=open_fn, \n",
    "    dict_transform=data_transform, \n",
    "    batch_size=bs, \n",
    "    workers=n_workers, \n",
    "    shuffle=True)\n",
    "\n",
    "valid_loader = UtilsFactory.create_loader(\n",
    "    valid_data, \n",
    "    open_fn=open_fn, \n",
    "    dict_transform=data_transform, \n",
    "    batch_size=bs, \n",
    "    workers=n_workers, \n",
    "    shuffle=False)\n",
    "\n",
    "loaders = collections.OrderedDict()\n",
    "loaders[\"train\"] = train_loader\n",
    "loaders[\"valid\"] = valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SegNet(num_classes=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# scheduler = None  # for OneCycle usage\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 40], gamma=0.3)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl.callbacks import (\n",
    "    ClassificationLossCallback, \n",
    "    BaseMetrics, Logger, TensorboardLogger,\n",
    "    OptimizerCallback, SchedulerCallback, CheckpointCallback, \n",
    "    PrecisionCallback, OneCycleLR)\n",
    "\n",
    "n_epochs = 50\n",
    "logdir = \"./logs/segmentation_notebook\"\n",
    "\n",
    "callbacks = collections.OrderedDict()\n",
    "\n",
    "callbacks[\"loss\"] = ClassificationLossCallback()\n",
    "callbacks[\"optimizer\"] = OptimizerCallback()\n",
    "callbacks[\"metrics\"] = BaseMetrics()\n",
    "\n",
    "# OneCylce custom scheduler callback\n",
    "callbacks[\"scheduler\"] = OneCycleLR(\n",
    "    cycle_len=n_epochs,\n",
    "    div=3, cut_div=4, momentum_range=(0.95, 0.85))\n",
    "\n",
    "# Pytorch scheduler callback\n",
    "# callbacks[\"scheduler\"] = SchedulerCallback(\n",
    "#     reduce_metric=\"loss_main\")\n",
    "\n",
    "callbacks[\"saver\"] = CheckpointCallback()\n",
    "callbacks[\"logger\"] = Logger()\n",
    "callbacks[\"tflogger\"] = TensorboardLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegNet(\n",
      "  (enc1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (enc2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (enc3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (enc4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (enc5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (8): ReLU(inplace)\n",
      "    (9): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (11): ReLU(inplace)\n",
      "    (12): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (dec5): Sequential(\n",
      "    (0): Upsample(scale_factor=2, mode=nearest)\n",
      "    (1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace)\n",
      "    (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (9): ReLU(inplace)\n",
      "    (10): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (12): ReLU(inplace)\n",
      "  )\n",
      "  (dec4): _DecoderBlock(\n",
      "    (decode): Sequential(\n",
      "      (0): Upsample(scale_factor=2, mode=nearest)\n",
      "      (1): Conv2d(1024, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace)\n",
      "      (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU(inplace)\n",
      "      (7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (9): ReLU(inplace)\n",
      "      (10): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (12): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (dec3): _DecoderBlock(\n",
      "    (decode): Sequential(\n",
      "      (0): Upsample(scale_factor=2, mode=nearest)\n",
      "      (1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace)\n",
      "      (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU(inplace)\n",
      "      (7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (9): ReLU(inplace)\n",
      "      (10): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (12): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (dec2): _DecoderBlock(\n",
      "    (decode): Sequential(\n",
      "      (0): Upsample(scale_factor=2, mode=nearest)\n",
      "      (1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace)\n",
      "      (4): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (6): ReLU(inplace)\n",
      "    )\n",
      "  )\n",
      "  (dec1): _DecoderBlock(\n",
      "    (decode): Sequential(\n",
      "      (0): Upsample(scale_factor=2, mode=nearest)\n",
      "      (1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (3): ReLU(inplace)\n",
      "      (4): Conv2d(64, 1, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (5): BatchNorm2d(1, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "BCEWithLogitsLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "<torch.optim.lr_scheduler.MultiStepLR object at 0x7efbe78e6128>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 * Epoch (train):   0% 0/63 [00:00<?, ?it/s]/home/ecohen/projects/miniconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "0 * Epoch (train): 100% 63/63 [00:58<00:00,  1.08it/s, batch time=11.01256, data time=0.00289, loss_main=0.69993, lr_main=0.00039, momentum_main=0.94199, sample per second=0.36322]\n",
      "0 * Epoch (valid): 100% 63/63 [00:12<00:00,  4.95it/s, batch time=0.12282, data time=0.00302, loss_main=0.69861, lr_main=0.00000, momentum_main=0.00000, sample per second=32.56858]\n",
      "[2018-10-20 10:31:11,179] 0 * Epoch (train) metrics: data time: 0.00874 | batch time: 0.77687 | sample per second: 10.72851 | lr_main: 0.00036 | momentum_main: 0.94593 | loss_main: 0.70824\n",
      "[2018-10-20 10:31:11,181] 0 * Epoch (valid) metrics: data time: 0.01236 | batch time: 0.20201 | sample per second: 20.57005 | lr_main: 0.00000 | momentum_main: 0.00000 | loss_main: 0.70087\n",
      "[2018-10-20 10:31:11,182] \n",
      "\n",
      "1 * Epoch (train): 100% 63/63 [00:32<00:00,  1.92it/s, batch time=0.21946, data time=0.00299, loss_main=0.69725, lr_main=0.00044, momentum_main=0.93386, sample per second=18.22640]\n",
      "1 * Epoch (valid): 100% 63/63 [00:12<00:00,  4.93it/s, batch time=0.12962, data time=0.00301, loss_main=0.69364, lr_main=0.00000, momentum_main=0.00000, sample per second=30.85845]\n",
      "[2018-10-20 10:32:01,306] 1 * Epoch (train) metrics: data time: 0.01134 | batch time: 0.37108 | sample per second: 10.96584 | lr_main: 0.00041 | momentum_main: 0.93780 | loss_main: 0.69704\n",
      "[2018-10-20 10:32:01,716] 1 * Epoch (valid) metrics: data time: 0.01040 | batch time: 0.20263 | sample per second: 20.33265 | lr_main: 0.00000 | momentum_main: 0.00000 | loss_main: 0.69375\n",
      "[2018-10-20 10:32:01,717] \n",
      "\n",
      "2 * Epoch (train):  57% 36/63 [00:19<00:14,  1.89it/s, batch time=0.36552, data time=0.00305, loss_main=0.69616, lr_main=0.00047, momentum_main=0.92916, sample per second=10.94342]Process Process-18:\n",
      "Process Process-17:\n",
      "Process Process-20:\n",
      "Process Process-19:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/ecohen/projects/miniconda3/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-6f187a1a35b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mlogdir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogdir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     epochs=n_epochs, verbose=True)\n\u001b[0m",
      "\u001b[0;32m/mnt/disk1/production/projects/varian-challenge/catalyst/dl/runner.py\u001b[0m in \u001b[0;36mtrain_stage\u001b[0;34m(self, loaders, callbacks, state_params, epochs, start_epoch, verbose, logdir)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m             verbose=verbose)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     def train(\n",
      "\u001b[0;32m/mnt/disk1/production/projects/varian-challenge/catalyst/dl/runner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, loaders, callbacks, state_params, epochs, start_epoch, mode, verbose)\u001b[0m\n\u001b[1;32m    171\u001b[0m                         state.output = self.batch_handler(\n\u001b[1;32m    172\u001b[0m                             dct=dct, model=self.model, state=state)\n\u001b[0;32m--> 173\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"on_batch_end\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/production/projects/varian-challenge/catalyst/dl/runner.py\u001b[0m in \u001b[0;36mrun_event\u001b[0;34m(self, callbacks, event)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{event}_pre\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m             \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m         \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"{event}_post\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/disk1/production/projects/varian-challenge/catalyst/dl/callbacks.py\u001b[0m in \u001b[0;36mon_batch_end\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34mf\"loss_{key}\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from catalyst.dl.runner import ClassificationRunner\n",
    "\n",
    "runner = ClassificationRunner(\n",
    "    model=model, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler)\n",
    "runner.train_stage(\n",
    "    loaders=loaders, \n",
    "    callbacks=callbacks, \n",
    "    logdir=logdir,\n",
    "    epochs=n_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
