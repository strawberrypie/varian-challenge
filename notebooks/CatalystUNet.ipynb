{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import collections\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from catalyst.data.augmentor import Augmentor\n",
    "from catalyst.utils.factory import UtilsFactory\n",
    "from catalyst.models.segmentation import UNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1488)\n",
    "n_images = 500\n",
    "bs = 4\n",
    "n_workers = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [np.random.uniform(high=256, size=(512, 512)).astype(np.uint8)\n",
    "          for x in range(n_images)]\n",
    "masks = [(np.random.uniform(high=1, size=(512, 512)) > 0.5).astype(float)\n",
    "         for x in range(n_images)]\n",
    "data = list(zip(images, masks))\n",
    "\n",
    "train_data = data[:n_images // 2]\n",
    "valid_data = data[n_images // 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving mock data\n",
    "data_dir = Path('..') / 'data'\n",
    "\n",
    "np.save(data_dir / 'mock_data_images', np.stack(images))\n",
    "np.save(data_dir / 'mock_data_masks', np.stack(masks))\n",
    "\n",
    "(data_dir / 'mock_data_images').mkdir(exist_ok=True)\n",
    "(data_dir / 'mock_data_images' / 'train').mkdir(exist_ok=True)\n",
    "(data_dir / 'mock_data_images' / 'valid').mkdir(exist_ok=True)\n",
    "\n",
    "(data_dir / 'mock_data_masks').mkdir(exist_ok=True)\n",
    "(data_dir / 'mock_data_masks' / 'train').mkdir(exist_ok=True)\n",
    "(data_dir / 'mock_data_masks' / 'valid').mkdir(exist_ok=True)\n",
    "\n",
    "for i, (image, mask) in enumerate(train_data):\n",
    "    Image.fromarray(image).save(data_dir / 'mock_data_images' / 'train' / (str(i) + '.tiff'))\n",
    "    Image.fromarray(mask).save(data_dir / 'mock_data_masks' / 'train' / (str(i) + '.tiff'))\n",
    "    \n",
    "for i, (image, mask) in enumerate(valid_data):\n",
    "    Image.fromarray(image).save(data_dir / 'mock_data_images' / 'valid' / (str(i) + '.tiff'))\n",
    "    Image.fromarray(mask).save(data_dir / 'mock_data_masks' / 'valid' / (str(i) + '.tiff'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    # TODO specify augmentations (e.g. histogram normalization)\n",
    "    Augmentor(\n",
    "        dict_key=\"features\",\n",
    "        augment_fn=lambda x: \\\n",
    "            torch.from_numpy(x.copy().astype(np.float32) / 256.).unsqueeze_(0).float()),\n",
    "    Augmentor(\n",
    "        dict_key=\"targets\",\n",
    "        augment_fn=lambda x: \\\n",
    "            torch.from_numpy(x.copy()).unsqueeze_(0).float()),\n",
    "])\n",
    "\n",
    "open_fn = lambda x: {\"features\": x[0], \"targets\": x[1]}\n",
    "\n",
    "\n",
    "train_loader = UtilsFactory.create_loader(\n",
    "    train_data, \n",
    "    open_fn=open_fn, \n",
    "    dict_transform=data_transform, \n",
    "    batch_size=bs, \n",
    "    workers=n_workers, \n",
    "    shuffle=True)\n",
    "\n",
    "valid_loader = UtilsFactory.create_loader(\n",
    "    valid_data, \n",
    "    open_fn=open_fn, \n",
    "    dict_transform=data_transform, \n",
    "    batch_size=bs, \n",
    "    workers=n_workers, \n",
    "    shuffle=False)\n",
    "\n",
    "loaders = collections.OrderedDict()\n",
    "loaders[\"train\"] = train_loader\n",
    "loaders[\"valid\"] = valid_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(in_channels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "# scheduler = None  # for OneCycle usage\n",
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10, 20, 40], gamma=0.3)\n",
    "# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=2, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from catalyst.dl.callbacks import (\n",
    "    ClassificationLossCallback, \n",
    "    BaseMetrics, Logger, TensorboardLogger,\n",
    "    OptimizerCallback, SchedulerCallback, CheckpointCallback, \n",
    "    PrecisionCallback, OneCycleLR)\n",
    "\n",
    "n_epochs = 50\n",
    "logdir = \"./logs/segmentation_notebook\"\n",
    "\n",
    "callbacks = collections.OrderedDict()\n",
    "\n",
    "callbacks[\"loss\"] = ClassificationLossCallback()\n",
    "callbacks[\"optimizer\"] = OptimizerCallback()\n",
    "callbacks[\"metrics\"] = BaseMetrics()\n",
    "\n",
    "# OneCylce custom scheduler callback\n",
    "callbacks[\"scheduler\"] = OneCycleLR(\n",
    "    cycle_len=n_epochs,\n",
    "    div=3, cut_div=4, momentum_range=(0.95, 0.85))\n",
    "\n",
    "# Pytorch scheduler callback\n",
    "# callbacks[\"scheduler\"] = SchedulerCallback(\n",
    "#     reduce_metric=\"loss_main\")\n",
    "\n",
    "callbacks[\"saver\"] = CheckpointCallback()\n",
    "callbacks[\"logger\"] = Logger()\n",
    "callbacks[\"tflogger\"] = TensorboardLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet(\n",
      "  (encoder): Encoder(\n",
      "    (block1): EncoderBlock(\n",
      "      (block): Sequential(\n",
      "        (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu1): ReLU()\n",
      "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (block2): EncoderBlock(\n",
      "      (block): Sequential(\n",
      "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu1): ReLU()\n",
      "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (block3): EncoderBlock(\n",
      "      (block): Sequential(\n",
      "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu1): ReLU()\n",
      "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu2): ReLU()\n",
      "      )\n",
      "    )\n",
      "    (pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (block4): EncoderBlock(\n",
      "      (block): Sequential(\n",
      "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu1): ReLU()\n",
      "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (relu2): ReLU()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (block3): DecoderBlock(\n",
      "      (uppool): Upsample(scale_factor=2, mode=bilinear)\n",
      "      (upconv): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (block2): DecoderBlock(\n",
      "      (uppool): Upsample(scale_factor=2, mode=bilinear)\n",
      "      (upconv): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "    (block1): DecoderBlock(\n",
      "      (uppool): Upsample(scale_factor=2, mode=bilinear)\n",
      "      (upconv): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    )\n",
      "  )\n",
      "  (final): Conv2d(64, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      ")\n",
      "BCEWithLogitsLoss()\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    eps: 1e-08\n",
      "    initial_lr: 0.001\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "<torch.optim.lr_scheduler.MultiStepLR object at 0x7f02a57507b8>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0 * Epoch (train):   0% 0/63 [00:00<?, ?it/s]/home/ecohen/projects/miniconda3/lib/python3.6/site-packages/torch/nn/modules/upsampling.py:122: UserWarning: nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.Upsampling is deprecated. Use nn.functional.interpolate instead.\")\n",
      "0 * Epoch (train):  43% 27/63 [00:20<00:27,  1.30it/s, batch time=0.13417, data time=0.00308, loss_main=0.69315, lr_main=0.00036, momentum_main=0.94657, sample per second=29.81275]"
     ]
    }
   ],
   "source": [
    "from catalyst.dl.runner import ClassificationRunner\n",
    "\n",
    "runner = ClassificationRunner(\n",
    "    model=model, \n",
    "    criterion=criterion, \n",
    "    optimizer=optimizer, \n",
    "    scheduler=scheduler)\n",
    "runner.train_stage(\n",
    "    loaders=loaders, \n",
    "    callbacks=callbacks, \n",
    "    logdir=logdir,\n",
    "    epochs=n_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
